{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Source_code_dissertation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQkJPjyadse0yUNie/l4x1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mufid99/Population_Coding_Thesis/blob/master/Source_code_dissertation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWlNdvERP8ye",
        "outputId": "33de2e76-6c0f-4dcf-bd78-bde377ceff25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "import csv\n",
        "import operator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import numbers\n",
        "import functools\n",
        "import operator\n",
        "\n",
        "'''\n",
        "Insert any of these datasets into the datasets array:\n",
        "\n",
        "'1027_ESL', '1028_SWD', '1029_LEV', '1030_ERA', '1089_USCrime', \n",
        "'1096_FacultySalaries', '192_vineyard', '195_auto_price', '207_autoPrice', \n",
        "'210_cloud', '228_elusage', '230_machine_cpu', '485_analcatdata_vehicle', \n",
        "'519_vinnie', '522_pm10', '523_analcatdata_neavote', \n",
        "'527_analcatdata_election2000', '542_pollution', '547_no2', \n",
        "'556_analcatdata_apnea2', '557_analcatdata_apnea1', '561_cpu', \n",
        "'579_fri_c0_250_5', '581_fri_c3_500_25', '582_fri_c1_500_25', \n",
        "'583_fri_c1_1000_50', '584_fri_c4_500_25', '586_fri_c3_1000_25', \n",
        "'588_fri_c4_1000_100', '589_fri_c2_1000_25', '590_fri_c0_1000_50'\n",
        ", '591_fri_c1_100_10', '592_fri_c4_1000_25', '593_fri_c1_1000_10', \n",
        "'594_fri_c2_100_5', '595_fri_c0_1000_10', '596_fri_c2_250_5', \n",
        "'597_fri_c2_500_5', '598_fri_c0_1000_25', '599_fri_c2_1000_5', \n",
        "'601_fri_c1_250_5', '602_fri_c3_250_10', '603_fri_c0_250_50', \n",
        "'604_fri_c4_500_10', '605_fri_c2_250_25', '606_fri_c2_1000_10', \n",
        "'607_fri_c4_1000_50', '608_fri_c3_1000_10', '609_fri_c0_1000_5', \n",
        "'611_fri_c3_100_5', '612_fri_c1_1000_5', '613_fri_c3_250_5', \n",
        "'615_fri_c4_250_10', '616_fri_c4_500_50', '617_fri_c3_500_5', \n",
        "'618_fri_c3_1000_50', '620_fri_c1_1000_25', '621_fri_c0_100_10', \n",
        "'622_fri_c2_1000_50', '623_fri_c4_1000_10', '624_fri_c0_100_5', \n",
        "'626_fri_c2_500_50', '627_fri_c2_500_10', '628_fri_c3_1000_5', \n",
        "'631_fri_c1_500_5', '633_fri_c0_500_25', '634_fri_c2_100_10', \n",
        "'635_fri_c0_250_10', '637_fri_c1_500_50', '641_fri_c1_500_10', \n",
        "'643_fri_c2_500_25', '644_fri_c4_250_25', '645_fri_c3_500_50', \n",
        "'646_fri_c3_500_10', '647_fri_c1_250_10', '648_fri_c1_250_50', \n",
        "'649_fri_c0_500_5', '650_fri_c0_500_50', '651_fri_c0_100_25', \n",
        "'653_fri_c0_250_25', '654_fri_c0_500_10', '656_fri_c1_100_5', \n",
        "'657_fri_c2_250_10', '658_fri_c3_250_25', '659_sleuth_ex1714', \n",
        "'663_rabe_266', '665_sleuth_case2002', '666_rmftsa_ladata', \n",
        "'678_visualizing_environmental', '687_sleuth_ex1605', '690_visualizing_galaxy', \n",
        "'695_chatfield_4', '706_sleuth_case1202', '712_chscase_geyser1'\n",
        "'''\n",
        "\n",
        "# datasets to benchmark the algorithm against\n",
        "# any combination of the above datasets can be inserted into this array\n",
        "datasets = ['601_fri_c1_250_5', '656_fri_c1_100_5']\n",
        "\n",
        "# turn input coding and output coding on and off\n",
        "CODE_INPUTS = True\n",
        "CODE_OUTPUTS = True\n",
        "\n",
        "# select of number of Gaussian centres for inputs or outputs\n",
        "# if either of the two constants above are false, then the corresponding number \n",
        "# of centres are not used in the code below\n",
        "NUM_GAUSSIAN_CENTRES_X = 7\n",
        "NUM_GAUSSIAN_CENTRES_Y = 10\n",
        "\n",
        "\n",
        "# Transforms dataset string into url\n",
        "def datasets_to_urls(datasets):\n",
        "  left_part = \"https://github.com/EpistasisLab/penn-ml-benchmarks/blob/master/datasets/regression/\"\n",
        "  right_part = \".tsv.gz?raw=true\"\n",
        "  urls = [left_part + dataset + \"/\" + dataset + right_part for dataset in datasets]\n",
        "  return urls\n",
        "\n",
        "# calculates Gaussian centres and standard deviations of dataset using a\n",
        "# a gaussian mixture model and returns both\n",
        "def get_gmm_gcs_stds(data, num_gcs):\n",
        "  gaussian_centres = []\n",
        "  stds = []\n",
        "  # loops over each feature\n",
        "  for i in range(len(data[0])):\n",
        "    num_comp = num_gcs\n",
        "    # checks the number of unique datapoints in a feature\n",
        "    unique_comp_len = len(np.unique(data[:,i]))\n",
        "    # if the number of unique datapoints are less than the number of components\n",
        "    # then the number of Gaussian centres is set equal to the number of datapoints\n",
        "    if unique_comp_len < num_comp:\n",
        "      num_comp = unique_comp_len\n",
        "    gm = GaussianMixture(n_components=num_comp)\n",
        "    gm.fit(data[:,i].reshape(-1,1))\n",
        "    # adjusting the dimensions of the arrays\n",
        "    means = functools.reduce(operator.iconcat,gm.means_,[])\n",
        "    variances = gm.covariances_.flatten()\n",
        "    # remove centres close in value\n",
        "    means, variances = remove_centres(means, variances, np.ptp(data[:,i]))\n",
        "    gaussian_centres.append(np.array(means))\n",
        "    stds.append(np.sqrt(variances))\n",
        "  return gaussian_centres, stds\n",
        "\n",
        "# removes centres that are close in value to each other\n",
        "def remove_centres(means, variances, values_range):\n",
        "  index = 0\n",
        "  while index < len(means):\n",
        "    index2 = index + 1\n",
        "    while index2 < len(means):\n",
        "      # if two means are within a tenth of the range\n",
        "      if abs(means[index] - means[index2]) <= (0.1 * values_range):\n",
        "        means.pop(index2)\n",
        "        variances = np.delete(variances, index2)\n",
        "      index2 += 1\n",
        "    index += 1\n",
        "  return means, variances\n",
        "\n",
        "# calculates gaussian centres using the intervals of the range of a dataset also \n",
        "# calculates the standard deviation of the dataset and returns both\n",
        "def get_interval_gcs_std(data, n_cent):\n",
        "  data_range = np.ptp(data)\n",
        "  # the difference between 2 consecutive intervals\n",
        "  increm = data_range/(n_cent-1)\n",
        "  # the first centre is the minumum value in the range\n",
        "  cur_cen = np.min(data)\n",
        "  gaussian_centres = []\n",
        "  for i in range(n_cent):\n",
        "    gaussian_centres.append(cur_cen)\n",
        "    cur_cen += increm\n",
        "  std = np.std(data)\n",
        "  return [np.array(gaussian_centres)], [std]\n",
        "\n",
        "# given a set of gaussian centres and standard deviations,\n",
        "# codes a value into multiple new values representing new features\n",
        "def code(mu, s, sigma):\n",
        "  # if mu is not a number a row with as many 0s as the number of centres is returned\n",
        "  if not isinstance(mu, numbers.Number):\n",
        "      return np.zeros((1, len(s)))\n",
        "  # checks if there are multiple standard deviations\n",
        "  if isinstance(sigma, (list, np.ndarray)):\n",
        "    for i in sigma:\n",
        "      # replace std with 1e-6 if it is smaller\n",
        "      i = max(1e-6, i)\n",
        "  else:\n",
        "    sigma = max(1e-6, sigma)\n",
        "  p1 = np.divide(0.5, (np.square(sigma))) \n",
        "  p2 = np.subtract(mu, s)\n",
        "  p3 = np.square(p2)\n",
        "  p4 = np.multiply(-p1, p3)\n",
        "  z = np.exp(p4)\n",
        "  return z\n",
        "\n",
        "# Applies coding to dataset passed using the Gaussian centres and stds\n",
        "def code_data(data, gaussian_centres, stds):\n",
        "  # new coded data of the neural network (new X or y)\n",
        "  coded_data = []\n",
        "  # each row in the dataset\n",
        "  for m in range(len(data)):\n",
        "      coded_data.append([])\n",
        "      # each column\n",
        "      for i in range(len(data[m])):\n",
        "          # makes sure there is more than 1 centre to apply coding\n",
        "          if gaussian_centres[i].size > 1:\n",
        "              coded = code(data[m][i], gaussian_centres[i], stds[i])\n",
        "              for j in coded:\n",
        "                  coded_data[m].append(j)\n",
        "          else:\n",
        "              coded_data[m].append(data[m][i])\n",
        "  return coded_data\n",
        "\n",
        "# decodes data to original value given the gaussian \n",
        "# centres used to code the data\n",
        "def decode(data, gaussian_centres):\n",
        "  new_data = []\n",
        "  for i in range(len(data)):\n",
        "    total = np.sum(data[i])\n",
        "    prod = np.sum(np.multiply(data[i], gaussian_centres[0]))\n",
        "    new_data.append([prod/total])\n",
        "  return np.array(new_data)\n",
        "\n",
        "urls = datasets_to_urls(datasets) \n",
        "\n",
        "'''\n",
        "Code beyond this point that does not involve population coding (calling the above\n",
        "functions) or writing to a csv is borrowed from the regression-benchmark repository.\n",
        "\n",
        "Author: Orzechowski, Patryk \n",
        "Date: 2018\n",
        "https://github.com/EpistasisLab/regression-benchmark/blob/master/scikit/validation-MLPRegressor.py\n",
        "'''\n",
        "\n",
        "for url in urls:\n",
        "  input_data = pd.read_csv(url, compression='gzip', sep='\\t')\n",
        "\n",
        "  TARGET_NAME = 'target'\n",
        "  INPUT_SEPARATOR = '\\t'\n",
        "  n_jobs = 1\n",
        "    \n",
        "  hyper_params = [\n",
        "      {\n",
        "          'activation' : ('logistic', 'tanh', 'relu',),\n",
        "          'solver' : ('lbfgs','adam','sgd',),\n",
        "          'learning_rate' : ('constant', 'invscaling', 'adaptive',),\n",
        "      },\n",
        "  ]\n",
        "\n",
        "  sc_y = StandardScaler()\n",
        "  X = StandardScaler().fit_transform(input_data.drop(TARGET_NAME, axis=1).values.astype(float))\n",
        "  y = sc_y.fit_transform(input_data[TARGET_NAME].values.reshape(-1,1))\n",
        "\n",
        "  data_titles = [\"Trial Number\", \"Best Params\", \"Train Score mse\", \n",
        "                 \"Train Score mae\", \"Test Score mse\", \"Test Score mae\", \"Runtime\"]\n",
        "  with open(url.split('/')[-1][:-16]+ \".csv\", 'w', newline='') as file:\n",
        "      wr = csv.writer(file)\n",
        "      wr.writerow(data_titles)\n",
        "\n",
        "  for i in range(1, 11):\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                        train_size=0.75,\n",
        "                                                        test_size=0.25,\n",
        "                                                        random_state=None)\n",
        "    \n",
        "    if CODE_INPUTS:\n",
        "      gaussian_centres_X, stds_X = get_gmm_gcs_stds(X_train, NUM_GAUSSIAN_CENTRES_X)\n",
        "      X_train = np.array(code_data(X_train, gaussian_centres_X, stds_X))\n",
        "      X_test = np.array(code_data(X_test, gaussian_centres_X, stds_X))\n",
        "    \n",
        "    if CODE_OUTPUTS:\n",
        "      gaussian_centres_y, stds_y = get_interval_gcs_std(y_train, NUM_GAUSSIAN_CENTRES_Y)\n",
        "      y_train_coded = np.array(code_data(y_train, gaussian_centres_y, stds_y))  \n",
        "\n",
        "    est=MLPRegressor()\n",
        "    grid_clf = GridSearchCV(est,cv=5,param_grid=hyper_params,\n",
        "                      verbose=0,n_jobs=n_jobs,scoring='r2')\n",
        "\n",
        "    t0 = time.time()\n",
        "    if CODE_OUTPUTS:\n",
        "      #fit model\n",
        "      grid_clf.fit(X_train, y_train_coded)\n",
        "      #get fit time\n",
        "      runtime = time.time()-t0\n",
        "      train_score_mse = mean_squared_error(sc_y.inverse_transform(y_train),\n",
        "      sc_y.inverse_transform\n",
        "      (decode(grid_clf.predict(X_train), gaussian_centres_y)))\n",
        "      train_score_mae = mean_absolute_error(sc_y.inverse_transform(y_train),\n",
        "      sc_y.inverse_transform\n",
        "      (decode(grid_clf.predict(X_train), gaussian_centres_y)))\n",
        "      test_score_mse = mean_squared_error(sc_y.inverse_transform(y_test),\n",
        "      sc_y.inverse_transform\n",
        "      (decode(grid_clf.predict(X_test), gaussian_centres_y)))\n",
        "      test_score_mae = mean_absolute_error(sc_y.inverse_transform(y_test),\n",
        "      sc_y.inverse_transform\n",
        "      (decode(grid_clf.predict(X_test), gaussian_centres_y)))\n",
        "    else:\n",
        "      #fit model\n",
        "      grid_clf.fit(X_train,y_train.ravel())\n",
        "      #get fit time\n",
        "      runtime = time.time()-t0\n",
        "      train_score_mse = mean_squared_error(sc_y.inverse_transform(y_train),\n",
        "      sc_y.inverse_transform(grid_clf.predict(X_train)))\n",
        "      train_score_mae = mean_absolute_error(sc_y.inverse_transform(y_train),\n",
        "      sc_y.inverse_transform(grid_clf.predict(X_train)))\n",
        "      test_score_mse = mean_squared_error(sc_y.inverse_transform(y_test),\n",
        "      sc_y.inverse_transform(grid_clf.predict(X_test)))\n",
        "      test_score_mae = mean_absolute_error(sc_y.inverse_transform(y_test),\n",
        "      sc_y.inverse_transform(grid_clf.predict(X_test)))\n",
        "\n",
        "    sorted_grid_params = sorted(grid_clf.best_params_.items(), key=operator.itemgetter(0))\n",
        "\n",
        "    # print results\n",
        "    out_text = '\\t'.join([url.split('/')[-1][:-7],\n",
        "                          'pop-cod',\n",
        "                          str(i),\n",
        "                          str(sorted_grid_params).replace('\\n',','),\n",
        "                          str(train_score_mse),\n",
        "                          str(train_score_mae),\n",
        "                          str(test_score_mse),\n",
        "                          str(test_score_mae),\n",
        "                          str(runtime)])\n",
        "\n",
        "    data = [str(i), str(sorted_grid_params).replace('\\n',','), str(train_score_mse), str(train_score_mae), str(test_score_mse), str(test_score_mae), str(runtime)]\n",
        "    with open(url.split('/')[-1][:-16]+ \".csv\", 'a+', newline='') as file:\n",
        "        wr = csv.writer(file)\n",
        "        wr.writerow(data)\n",
        "\n",
        "    print(out_text)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-40594de3916b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m   \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m   \u001b[0mTARGET_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 642\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ]
    }
  ]
}